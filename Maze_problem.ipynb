{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the grid (0: open cell, 1: wall)\n",
        "grid = [\n",
        "    [0, 0, 1, 0, 0],\n",
        "    [0, 1, 0, 1, 0],\n",
        "    [0, 0, 0, 0, 0],\n",
        "    [1, 1, 1, 1, 0],\n",
        "    [0, 0, 0, 1, 0]\n",
        "]\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.1\n",
        "discount_factor = 0.9\n",
        "exploration_rate = 1.0\n",
        "exploration_decay = 0.99\n",
        "min_exploration_rate = 0.1\n",
        "num_episodes = 1000\n",
        "\n",
        "# Initialize Q-table\n",
        "n_states = len(grid) * len(grid[0])\n",
        "n_actions = 4  # Up, Down, Left, Right\n",
        "q_table = np.zeros((n_states, n_actions))\n",
        "\n",
        "def state_to_index(row, col):\n",
        "    return row * len(grid[0]) + col\n",
        "\n",
        "def index_to_state(index):\n",
        "    return divmod(index, len(grid[0]))\n",
        "\n",
        "def get_available_actions(state):\n",
        "    row, col = index_to_state(state)\n",
        "    actions = []\n",
        "    if row > 0 and grid[row - 1][col] == 0:  # Up\n",
        "        actions.append(0)\n",
        "    if row < len(grid) - 1 and grid[row + 1][col] == 0:  # Down\n",
        "        actions.append(1)\n",
        "    if col > 0 and grid[row][col - 1] == 0:  # Left\n",
        "        actions.append(2)\n",
        "    if col < len(grid[0]) - 1 and grid[row][col + 1] == 0:  # Right\n",
        "        actions.append(3)\n",
        "    return actions\n",
        "\n",
        "def train_agent():\n",
        "    global exploration_rate\n",
        "    for episode in range(num_episodes):\n",
        "        state = state_to_index(0, 0)  # Start position\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.uniform(0, 1) < exploration_rate:\n",
        "                action = random.choice(get_available_actions(state))  # Explore\n",
        "            else:\n",
        "                action = np.argmax(q_table[state])  # Exploit\n",
        "\n",
        "            # Determine new position based on action\n",
        "            new_row, new_col = index_to_state(state)\n",
        "            if action == 0:  # Up\n",
        "                new_row -= 1\n",
        "            elif action == 1:  # Down\n",
        "                new_row += 1\n",
        "            elif action == 2:  # Left\n",
        "                new_col -= 1\n",
        "            elif action == 3:  # Right\n",
        "                new_col += 1\n",
        "\n",
        "            # Check if the new position is valid\n",
        "            if 0 <= new_row < len(grid) and 0 <= new_col < len(grid[0]) and grid[new_row][new_col] == 0:\n",
        "                new_state = state_to_index(new_row, new_col)\n",
        "            else:\n",
        "                new_state = state  # If invalid, stay in the same state\n",
        "\n",
        "            reward = -1  # Negative cost for each move\n",
        "            if (new_row, new_col) == (len(grid) - 1, len(grid[0]) - 1):\n",
        "                reward = 10  # Reward for reaching the goal\n",
        "                done = True  # End episode\n",
        "\n",
        "            # Update Q-value\n",
        "            best_future_q = np.max(q_table[new_state])\n",
        "            q_table[state][action] += learning_rate * (reward + discount_factor * best_future_q - q_table[state][action])\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "        # Decay exploration rate\n",
        "        if exploration_rate > min_exploration_rate:\n",
        "            exploration_rate *= exploration_decay\n",
        "\n",
        "train_agent()\n",
        "\n",
        "# Display Q-table\n",
        "print(\"Q-table:\")\n",
        "print(q_table)\n",
        "\n",
        "# Test the learned policy\n",
        "def test_agent():\n",
        "    state = state_to_index(0, 0)\n",
        "    path = []\n",
        "\n",
        "    while state != state_to_index(len(grid) - 1, len(grid[0]) - 1):\n",
        "        path.append(state)\n",
        "        action = np.argmax(q_table[state])\n",
        "        new_row, new_col = index_to_state(state)\n",
        "\n",
        "        if action == 0:  # Up\n",
        "            new_row -= 1\n",
        "        elif action == 1:  # Down\n",
        "            new_row += 1\n",
        "        elif action == 2:  # Left\n",
        "            new_col -= 1\n",
        "        elif action == 3:  # Right\n",
        "            new_col += 1\n",
        "\n",
        "        # Check if the new position is valid\n",
        "        if 0 <= new_row < len(grid) and 0 <= new_col < len(grid[0]) and grid[new_row][new_col] == 0:\n",
        "            state = state_to_index(new_row, new_col)\n",
        "\n",
        "    path.append(state)  # Add the goal state\n",
        "    return path\n",
        "\n",
        "# Get the path from start to finish\n",
        "path = test_agent()\n",
        "print(\"Learned path:\", path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhD_-m5qJSbR",
        "outputId": "019159b6-2252-459b-fbf9-6b5870b1b13b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-table:\n",
            "[[-2.37657286 -0.434062   -2.37657286 -2.2637506 ]\n",
            " [-2.14321859 -2.14321859 -1.39258132 -2.14321859]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.4900995  -0.3940399  -0.3940399  -0.39862095]\n",
            " [-0.3940399   2.64371958 -1.12375735 -0.3940399 ]\n",
            " [-1.3943233   0.62882    -1.9836941  -1.90272132]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.199       3.11861116 -0.199      -0.199     ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.10174936  6.19906794 -0.1         0.        ]\n",
            " [-0.43797431 -1.31254187 -1.22478977  1.8098    ]\n",
            " [-0.58519851 -0.58519851  0.62813428  3.122     ]\n",
            " [ 1.78654491 -0.67934652  1.80381424  4.58      ]\n",
            " [-0.1         0.          3.12108081  6.2       ]\n",
            " [ 4.57197077  8.          4.57889704  0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 6.19977513 10.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]]\n",
            "Learned path: [0, 5, 10, 11, 12, 13, 14, 19, 24]\n"
          ]
        }
      ]
    }
  ]
}